<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="webthemez">
    <title>Burghardt Lab</title>
	<!-- core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="css/animate.min.css" rel="stylesheet">
    <link href="css/prettyPhoto.css" rel="stylesheet">
    <link href="css/styles.css" rel="stylesheet"> 
    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <script src="js/respond.min.js"></script>
    <![endif]-->       
    <link rel="shortcut icon" href="images/ico/favicon.ico"> 
</head> 

<body id="home">
  
   <section id="hero-banner2">
             <div class="banner-inner">
                    <div class="container">
                        <div class="row">
                            <div class="col-sm-8">
                                    <h2><a href="https://arxiv.org/abs/2209.08697">Quantifying How Hateful Communities Radicalize Online Users</a></h2><br>
				    <h3>Matheus Schmitz, Keith Burghardt, Goran Muric</h3>
				    While online social media offers a way for ignored or stifled voices to be heard, it also allows users a platform to spread hateful speech. Such speech usually originates in fringe communities, yet it can spill over into mainstream channels. In this paper, we measure the impact of joining fringe hateful communities in terms of hate speech propagated to the rest of the social network. We leverage data from Reddit to assess the effect of joining one type of echo chamber: a digital community of like-minded users exhibiting hateful behavior. We measure members' usage of hate speech outside the studied community before and after they become active participants. Using Interrupted Time Series (ITS) analysis as a causal inference method, we gauge the spillover effect, in which hateful language from within a certain community can spread outside that community by using the level of out-of-community hate word usage as a proxy for learned hate. We investigate four different Reddit sub-communities (subreddits) covering three areas of hate speech: racism, misogyny and fat-shaming. In all three cases we find an increase in hate speech outside the originating community, implying that joining such community leads to a spread of hate speech throughout the platform. Moreover, users are found to pick up this new hateful speech for months after initially joining the community. We show that the harmful speech does not remain contained within the community. Our results provide new evidence of the harmful effects of echo chambers and the potential benefit of moderating them to reduce adoption of hateful speech.
				    <a class="btn btn-primary btn-lg" href="#research">A Deeper Dive</a>
                            </div>
                <div class="col-sm-8 wow fadeInLeft">
                  <img class="img-responsive" src="images/hate_spread/hate01.jpg" width=300px alt="">
                </div>
                        </div>	
		     </div>
		     </div>
</section>
     <section id="hero-banner2">
             <div class="banner-inner">
                    <div class="container">
                        <div class="row">
                            <div class="col-sm-8">
                                    <h2><a href="#">No Love Among Haters: Negative Interactions Reduce Hate Community Engagement</a></h2><br>
				    <h3>Daniel Hickey, Matheus Schmitz, Daniel Fessler, Paul Smaldino, Goran Muric, Keith Burghardt</h3>
				    While online hate groups pose significant risks to the health of online platforms and safety of marginalized groups, little is known about what causes users to become active in hate groups and the effect of social interactions on furthering their engagement. We address this gap by first developing tools to find hate communities within Reddit, and then augment 11 subreddits extracted with 14 known hateful subreddits (25 in total). Using causal inference methods, we evaluate the effect of replies on engagement in hateful subreddits by comparing users who receive replies to their first post (the treatment) to equivalent control users who did not. We find users with replies are \emph{less} likely to become engaged in hateful subreddits than users who do not, while the opposite effect is observed for a matched sample of similar-sized non-hateful subreddits. Using the Google Perspective API and VADER, we discover that hateful community first-repliers are much more toxic, negative, and attack the posters more often than non-hateful first-repliers. In addition, we uncover a negative correlation between engagement and attacks or toxicity of first-repliers. We simulate the cumulative engagement of hateful and non-hateful subreddits under the contra-positive scenario of friendly first-replies and find that attacks dramatically reduce engagement in hateful subreddits. These results counter-intuitively imply that, although under-moderated communities allow hate to fester, the resulting contexts are such that direct social interaction does not encourage further participation, thus endogenously constraining the harmful role that these communities could play as recruitment venues for antisocial beliefs.
				    <a class="btn btn-primary btn-lg" href="#research">A Deeper Dive</a>
                            </div>
                <div class="col-sm-8 wow fadeInLeft">
                  <img class="img-responsive" src="images/hate_spread/hate00.jpg" width=300px alt="">
                </div>
                        </div>	
		     </div>
		     </div>
</section>
  
  
  </body>
</html>
